# -*- coding: utf-8 -*-
"""
TextCNNæ¨¡å‹è®­ç»ƒå™¨
ç”¨é€”ï¼šä¸“é—¨è´Ÿè´£TextCNNæ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œä¿å­˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
from typing import Dict, List, Tuple, Any
from pathlib import Path
import json
from tqdm import tqdm

from ..utils.config import Config
from ..architectures.textcnn import TextCNN


class TextCNNTrainer:
    """
    TextCNNè®­ç»ƒå™¨ç±»
    ç”¨é€”ï¼šä¸“é—¨å¤„ç†TextCNNæ¨¡å‹çš„è®­ç»ƒæµç¨‹
    """
    
    def __init__(self, language: str = "chinese", vocab: Dict[str, int] = None):
        """
        åˆå§‹åŒ–TextCNNè®­ç»ƒå™¨
        å‚æ•°ï¼š
            language: è¯­è¨€ç±»å‹
            vocab: è¯æ±‡è¡¨
        """
        self.language = language
        self.vocab = vocab
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.optimizer = None
        self.criterion = None
        
        # è®­ç»ƒå†å²è®°å½•
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        print(f"åˆå§‹åŒ–TextCNNè®­ç»ƒå™¨ ({language}), è®¾å¤‡: {self.device}")
    
    def create_model(self) -> nn.Module:
        """
        åˆ›å»ºTextCNNæ¨¡å‹
        è¿”å›å€¼ï¼šTextCNNæ¨¡å‹å®ä¾‹
        """
        if self.vocab is None:
            raise ValueError("è¯æ±‡è¡¨ä¸èƒ½ä¸ºç©º")
        
        vocab_size = len(self.vocab)
        self.model = TextCNN.create_model(vocab_size)
        self.model.to(self.device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=Config.TRAINING_CONFIG['learning_rate']
        )
        self.criterion = nn.CrossEntropyLoss()
        
        print(f"TextCNNæ¨¡å‹åˆ›å»ºå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size}")
        return self.model
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        å‚æ•°ï¼š
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        è¿”å›å€¼ï¼š(å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        self.model.train()
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(input_ids)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = correct_predictions / total_predictions
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.4f}'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / total_predictions
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch
        å‚æ•°ï¼š
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        è¿”å›å€¼ï¼š(å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        self.model.eval()
        total_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯ä¸­"):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_predictions += labels.size(0)
                correct_predictions += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions
        
        return avg_loss, accuracy
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              epochs: int = 10, save_best: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        å‚æ•°ï¼š
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
        è¿”å›å€¼ï¼šè®­ç»ƒç»“æœå­—å…¸
        """
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒTextCNNæ¨¡å‹ï¼Œå…±{epochs}è½®...")
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_loader.dataset)}")
        print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        print("=" * 60)
        
        best_val_acc = 0.0
        best_model_state = None
        
        for epoch in range(epochs):
            print(f"\nğŸ”„ Epoch {epoch + 1}/{epochs}")
            print("-" * 40)
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            print(f"ğŸ“ˆ è®­ç»ƒç»“æœ:")
            print(f"   - è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"   - è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} ({train_acc*100:.2f}%)")
            print(f"   - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"   - éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_best and val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
                print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
            else:
                print(f"ğŸ“Š å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
        
        # æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡
        if save_best and best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print(f"\nâœ… æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.save_model()
        
        return {
            'model_type': 'textcnn',
            'language': self.language,
            'epochs': epochs,
            'best_val_accuracy': best_val_acc,
            'final_train_accuracy': self.history['train_acc'][-1],
            'final_val_accuracy': self.history['val_acc'][-1],
            'model_path': str(model_path),
            'history': self.history
        }
    
    def save_model(self) -> Path:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        è¿”å›å€¼ï¼šæ¨¡å‹ä¿å­˜è·¯å¾„
        """
        model_path = Config.get_model_path('textcnn', self.language)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'vocab': self.vocab,
            'model_config': Config.MODEL_CONFIGS['textcnn'],
            'language': self.language,
            'history': self.history
        }
        
        torch.save(checkpoint, model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        return model_path
    
    def evaluate(self, test_loader: DataLoader) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        å‚æ•°ï¼š
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        è¿”å›å€¼ï¼šè¯„ä¼°ç»“æœå­—å…¸
        """
        self.model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids)
                _, predicted = torch.max(outputs.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        report = classification_report(all_labels, all_predictions, output_dict=True)
        
        results = {
            'accuracy': accuracy,
            'classification_report': report,
            'total_samples': len(all_labels)
        }
        
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
        return results 