# -*- coding: utf-8 -*-
"""
è®­ç»ƒç®¡ç†å™¨
ç”¨é€”ï¼šç»Ÿä¸€ç®¡ç†ä¸‰ç§æ¨¡å‹ï¼ˆTextCNNã€BiLSTMã€BERTï¼‰çš„è®­ç»ƒæµç¨‹
"""

import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Any, Optional, Callable
from pathlib import Path
import json
from collections import Counter
from tqdm import tqdm

from ..utils.config import Config
from ..utils.text_processor import TextProcessor
from ..scripts.dataset_loader import DatasetLoader
from .textcnn_trainer import TextCNNTrainer
from .bilstm_trainer import BiLSTMTrainer
from .bert_trainer import BertTrainer


class SentimentDataset(Dataset):
    """
    æƒ…æ„Ÿåˆ†ææ•°æ®é›†ç±»
    ç”¨é€”ï¼šå°†é¢„å¤„ç†åçš„æ•°æ®è½¬æ¢ä¸ºPyTorch Datasetæ ¼å¼
    """
    
    def __init__(self, data: List[Dict], vocab: Dict = None, is_bert: bool = False):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        å‚æ•°ï¼š
            data: å¤„ç†åçš„æ•°æ®åˆ—è¡¨
            vocab: è¯æ±‡è¡¨ï¼ˆéBERTæ¨¡å‹éœ€è¦ï¼‰
            is_bert: æ˜¯å¦ä¸ºBERTæ¨¡å‹
        """
        self.data = data
        self.vocab = vocab
        self.is_bert = is_bert
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = self.data[idx]
        
        if self.is_bert:
            # BERTæ¨¡å‹æ•°æ®æ ¼å¼
            return {
                'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),
                'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),
                'labels': torch.tensor(item['label'], dtype=torch.long)
            }
        else:
            # ä¼ ç»Ÿæ¨¡å‹æ•°æ®æ ¼å¼
            return {
                'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),
                'labels': torch.tensor(item['label'], dtype=torch.long)
            }


class TrainerManager:
    """
    è®­ç»ƒç®¡ç†å™¨ç±»
    ç”¨é€”ï¼šç»Ÿä¸€ç®¡ç†ä¸åŒæ¨¡å‹çš„è®­ç»ƒæµç¨‹
    """
    
    def __init__(self, model_type: str, language: str = "chinese", 
                 progress_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨
        å‚æ•°ï¼š
            model_type: æ¨¡å‹ç±»å‹ (textcnn/bilstm/bert)
            language: è¯­è¨€ç±»å‹ (chinese/english)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        self.model_type = model_type
        self.language = language
        self.progress_callback = progress_callback
        self.text_processor = TextProcessor(language)
        
        # æ•°æ®ç›¸å…³
        self.vocab = None
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
        
        # è®­ç»ƒå™¨
        self.trainer = None
        
        # ç¡®ä¿æ”¯æŒçš„æ¨¡å‹ç±»å‹
        if model_type not in ["textcnn", "bilstm", "bert"]:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        print(f"åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨: {model_type} ({language})")
    
    def _update_progress(self, progress: int, message: str) -> None:
        """
        æ›´æ–°è®­ç»ƒè¿›åº¦
        å‚æ•°ï¼š
            progress: è¿›åº¦ç™¾åˆ†æ¯”
            message: è¿›åº¦æ¶ˆæ¯
        """
        if self.progress_callback:
            self.progress_callback(progress, message)
    
    def build_vocab(self, dataset, min_freq: int = 2) -> Dict[str, int]:
        """
        æ„å»ºè¯æ±‡è¡¨ï¼ˆä»…ç”¨äºä¼ ç»Ÿæ¨¡å‹ï¼‰
        å‚æ•°ï¼š
            dataset: è®­ç»ƒæ•°æ®é›†
            min_freq: æœ€å°è¯é¢‘é˜ˆå€¼
        è¿”å›å€¼ï¼šè¯æ±‡è¡¨å­—å…¸
        """
        print("æ„å»ºè¯æ±‡è¡¨...")
        word_counts = Counter()
        
        for example in tqdm(dataset, desc="ç»Ÿè®¡è¯é¢‘"):
            tokens = self.text_processor.tokenize(example["text"])
            word_counts.update(tokens)
        
        # åˆ›å»ºè¯æ±‡è¡¨
        vocab = {"<PAD>": 0, "<UNK>": 1}
        vocab_size = 2
        
        for word, count in word_counts.most_common():
            if count >= min_freq:
                vocab[word] = vocab_size
                vocab_size += 1
        
        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {len(vocab)}")
        return vocab
    
    def prepare_data(self, train_data, val_data, test_data, max_length: int = None) -> None:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        å‚æ•°ï¼š
            train_data, val_data, test_data: åŸå§‹æ•°æ®é›†
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        print(f"ğŸ”§ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®...")
        self._update_progress(20, "å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        if max_length is None:
            max_length = Config.MODEL_CONFIGS[self.model_type]['max_seq_length']
        
        print(f"   - æ¨¡å‹ç±»å‹: {self.model_type}")
        print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {Config.TRAINING_CONFIG['batch_size']}")
        
        if self.model_type == "bert":
            # BERTæ•°æ®é¢„å¤„ç†
            print(f"   - ä½¿ç”¨BERT tokenizerè¿›è¡Œæ•°æ®é¢„å¤„ç†")
            self._prepare_bert_data(train_data, val_data, test_data, max_length)
        else:
            # ä¼ ç»Ÿæ¨¡å‹æ•°æ®é¢„å¤„ç†
            print(f"   - æ„å»ºè¯æ±‡è¡¨å¹¶è¿›è¡Œæ•°æ®ç¼–ç ")
            self._prepare_traditional_data(train_data, val_data, test_data, max_length)
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    
    def _prepare_bert_data(self, train_data, val_data, test_data, max_length: int) -> None:
        """
        å‡†å¤‡BERTæ¨¡å‹æ•°æ®
        """
        from ..architectures.bert_model import BertTokenizerWrapper
        
        # åˆå§‹åŒ–BERT tokenizer
        model_name = "bert-base-chinese" if self.language == "chinese" else "bert-base-uncased"
        tokenizer = BertTokenizerWrapper(model_name)
        
        # å¤„ç†å„ä¸ªæ•°æ®é›†
        train_encoded = self._encode_bert_dataset(train_data, tokenizer, max_length)
        val_encoded = self._encode_bert_dataset(val_data, tokenizer, max_length)
        test_encoded = self._encode_bert_dataset(test_data, tokenizer, max_length)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = Config.TRAINING_CONFIG['batch_size']
        self.train_loader = DataLoader(
            SentimentDataset(train_encoded, is_bert=True),
            batch_size=batch_size, shuffle=True
        )
        self.val_loader = DataLoader(
            SentimentDataset(val_encoded, is_bert=True),
            batch_size=batch_size, shuffle=False
        )
        self.test_loader = DataLoader(
            SentimentDataset(test_encoded, is_bert=True),
            batch_size=batch_size, shuffle=False
        )
    
    def _prepare_traditional_data(self, train_data, val_data, test_data, max_length: int) -> None:
        """
        å‡†å¤‡ä¼ ç»Ÿæ¨¡å‹æ•°æ®
        """
        # æ„å»ºè¯æ±‡è¡¨
        self.vocab = self.build_vocab(train_data)
        
        # ç¼–ç æ•°æ®é›†
        train_encoded = self._encode_traditional_dataset(train_data, max_length)
        val_encoded = self._encode_traditional_dataset(val_data, max_length)
        test_encoded = self._encode_traditional_dataset(test_data, max_length)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = Config.TRAINING_CONFIG['batch_size']
        self.train_loader = DataLoader(
            SentimentDataset(train_encoded, self.vocab),
            batch_size=batch_size, shuffle=True
        )
        self.val_loader = DataLoader(
            SentimentDataset(val_encoded, self.vocab),
            batch_size=batch_size, shuffle=False
        )
        self.test_loader = DataLoader(
            SentimentDataset(test_encoded, self.vocab),
            batch_size=batch_size, shuffle=False
        )
    
    def _encode_bert_dataset(self, dataset, tokenizer, max_length: int) -> List[Dict]:
        """
        ç¼–ç BERTæ•°æ®é›†
        """
        texts = [example["text"] for example in dataset]
        labels = [example["label"] for example in dataset]
        
        # ä½¿ç”¨tokenizerç¼–ç 
        encoded = tokenizer.encode_texts(texts, max_length=max_length)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        encoded_data = []
        for i in range(len(texts)):
            encoded_data.append({
                'input_ids': encoded['input_ids'][i].tolist(),
                'attention_mask': encoded['attention_mask'][i].tolist(),
                'label': labels[i]
            })
        
        return encoded_data
    
    def _encode_traditional_dataset(self, dataset, max_length: int) -> List[Dict]:
        """
        ç¼–ç ä¼ ç»Ÿæ¨¡å‹æ•°æ®é›†
        """
        encoded_data = []
        
        for example in tqdm(dataset, desc="ç¼–ç æ•°æ®"):
            tokens = self.text_processor.tokenize(example["text"])
            
            # è½¬æ¢ä¸ºIDåºåˆ—
            token_ids = [self.vocab.get(token, self.vocab["<UNK>"]) for token in tokens]
            
            # æˆªæ–­æˆ–å¡«å……
            if len(token_ids) > max_length:
                token_ids = token_ids[:max_length]
            else:
                token_ids = token_ids + [self.vocab["<PAD>"]] * (max_length - len(token_ids))
            
            encoded_data.append({
                'input_ids': token_ids,
                'label': example["label"]
            })
        
        return encoded_data
    
    def train(self, epochs: int = None, learning_rate: float = None, 
              batch_size: int = None) -> Dict[str, Any]:
        """
        å¯åŠ¨è®­ç»ƒæµç¨‹
        å‚æ•°ï¼š
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            batch_size: æ‰¹æ¬¡å¤§å°
        è¿”å›å€¼ï¼šè®­ç»ƒç»“æœå­—å…¸
        """
        print(f"ğŸ¯ å¼€å§‹åˆ›å»ºè®­ç»ƒå™¨...")
        self._update_progress(30, "åˆ›å»ºè®­ç»ƒå™¨...")
        
        # ä½¿ç”¨é»˜è®¤å‚æ•°
        if epochs is None:
            epochs = Config.TRAINING_CONFIG['num_epochs']
            # BERTé€šå¸¸éœ€è¦è¾ƒå°‘çš„è½®æ•°
            if self.model_type == "bert":
                epochs = min(epochs, 3)
        
        print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   - è®­ç»ƒè½®æ•°: {epochs}")
        print(f"   - å­¦ä¹ ç‡: {learning_rate or Config.TRAINING_CONFIG['learning_rate']}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size or Config.TRAINING_CONFIG['batch_size']}")
        
        # åˆ›å»ºå¯¹åº”çš„è®­ç»ƒå™¨
        if self.model_type == "textcnn":
            self.trainer = TextCNNTrainer(self.language, self.vocab)
        elif self.model_type == "bilstm":
            self.trainer = BiLSTMTrainer(self.language, self.vocab)
        elif self.model_type == "bert":
            self.trainer = BertTrainer(self.language)
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸ åˆ›å»º{self.model_type}æ¨¡å‹...")
        self._update_progress(40, "åˆ›å»ºæ¨¡å‹...")
        model = self.trainer.create_model()
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        self._update_progress(50, "å¼€å§‹è®­ç»ƒ...")
        results = self.trainer.train(
            self.train_loader, 
            self.val_loader, 
            epochs=epochs, 
            save_best=True
        )
        
        # æµ‹è¯•æ¨¡å‹
        print(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹...")
        self._update_progress(90, "æµ‹è¯•æ¨¡å‹...")
        test_results = self.trainer.evaluate(self.test_loader)
        results['test_results'] = test_results
        
        print(f"âœ… è®­ç»ƒæµç¨‹å®Œæˆ")
        self._update_progress(100, "è®­ç»ƒå®Œæˆ")
        
        return results
    
    def load_data(self, max_samples: int = None) -> Tuple:
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        å‚æ•°ï¼š
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
        è¿”å›å€¼ï¼š(train_data, val_data, test_data)
        """
        print(f"ğŸ“‚ å¼€å§‹åŠ è½½{self.language}æ•°æ®é›†...")
        self._update_progress(10, "åŠ è½½æ•°æ®é›†...")
        
        # æ™ºèƒ½åŠ è½½æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®é›†ï¼‰
        loader = DatasetLoader(language=self.language)
        train_data, val_data, test_data = loader.get_or_download_data(max_samples)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"   - è®­ç»ƒé›†: {len(train_data)} æ¡")
        print(f"   - éªŒè¯é›†: {len(val_data)} æ¡") 
        print(f"   - æµ‹è¯•é›†: {len(test_data)} æ¡")
        
        return train_data, val_data, test_data
    
    def full_training_pipeline(self, epochs: int = None, learning_rate: float = None, 
                              batch_size: int = None, max_samples: int = None) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è®­ç»ƒæµæ°´çº¿
        å‚æ•°ï¼š
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            batch_size: æ‰¹æ¬¡å¤§å°
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡
        è¿”å›å€¼ï¼šè®­ç»ƒç»“æœå­—å…¸
        """
        try:
            # åŠ è½½æ•°æ®
            train_data, val_data, test_data = self.load_data(max_samples)
            
            # å‡†å¤‡æ•°æ®
            self.prepare_data(train_data, val_data, test_data)
            
            # è®­ç»ƒæ¨¡å‹
            results = self.train(epochs, learning_rate, batch_size)
            
            return results
            
        except Exception as e:
            error_msg = f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(error_msg)
            self._update_progress(-1, error_msg)
            raise e 